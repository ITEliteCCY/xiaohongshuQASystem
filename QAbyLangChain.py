import openai
openai_api_key = 'xxx' #openaiå®˜ç½‘è·å–
# openai.api_key = openai_api_key
openai_api_base = 'xxx' #åä»£ç†https://api.openai.comçš„åŸŸå
# openai.api_base = openai_api_base
openai_model_name="gpt-3.5-turbo"
from ProcessData import processData



from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ChatVectorDBChain, ConversationalRetrievalChain

from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate)


if __name__ == '__main__':

    #ç”¨æˆ·åœ¨å°çº¢è–¯ä¸Šå¤åˆ¶çš„é“¾æ¥
    inputXHSShareURL = "40 é»‘å¨ƒå‘¦ï½å‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ ğŸ˜† 4hydQI4uMU4D8yf ğŸ˜† http://xhslink.com/8VNHfuï¼Œå¤åˆ¶æœ¬æ¡ä¿¡æ¯ï¼Œæ‰“å¼€ã€å°çº¢ä¹¦ã€‘AppæŸ¥çœ‹ç²¾å½©å†…å®¹ï¼"

    #å¤„ç†é“¾æ¥ã€æ‹¿åˆ°ç½‘å€ã€æ‹¿åˆ°MP4URLã€ä¸‹è½½MP4ã€è½¬å½•åˆ°éŸ³é¢‘ã€åŸºäºopenaiçš„whisperæ¨¡å‹å°†éŸ³é¢‘è½¬å½•åˆ°æ–‡æœ¬
    text_path = processData(inputXHSShareURL)
    text_path = "/Users/caochongyang/Downloads/text/  01e4c2381a456fca010370038996ad241c_259.text"
    loader = TextLoader(text_path, encoding="utf-8")

    # å°†æ•°æ®è½¬æˆ document
    documents = loader.load()
    print(documents[0].page_content)

    # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=20
    )

    # åˆ†å‰² documents
    documents = text_splitter.split_documents(documents)

    # åˆå§‹åŒ– openai embeddings
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key,openai_api_base=openai_api_base)

    # å°†æ•°æ®å­˜å…¥å‘é‡å­˜å‚¨
    vector_store = Chroma.from_documents(documents, embeddings)
    # é€šè¿‡å‘é‡å­˜å‚¨åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = vector_store.as_retriever()

    system_template = """
    Use the following context to answer the user's question.
    If you don't know the answer, say you don't, don't try to make it up. And answer in Chinese.
    -----------
    {question}
    -----------
    {chat_history}
    """

    # æ„å»ºåˆå§‹ messages åˆ—è¡¨ï¼Œè¿™é‡Œå¯ä»¥ç†è§£ä¸ºæ˜¯ openai ä¼ å…¥çš„ messages å‚æ•°
    messages = [
        SystemMessagePromptTemplate.from_template(system_template),
        HumanMessagePromptTemplate.from_template('{question}')
    ]

    # åˆå§‹åŒ– prompt å¯¹è±¡
    prompt = ChatPromptTemplate.from_messages(messages)

    # åˆå§‹åŒ–é—®ç­”é“¾
    qa = ConversationalRetrievalChain.from_llm(
        ChatOpenAI(temperature=0, max_tokens=2048,openai_api_key=openai_api_key,openai_api_base=openai_api_base,model_name=openai_model_name),
        retriever,
        condense_question_prompt=prompt
    )

    chat_history = []
    while True:
        question = input('é—®é¢˜ï¼š')
        # å¼€å§‹å‘é€é—®é¢˜ chat_history ä¸ºå¿…é¡»å‚æ•°,ç”¨äºå­˜å‚¨å¯¹è¯å†å²
        result = qa({'question': question, 'chat_history': chat_history})
        chat_history.append((question, result['answer']))
        print(result['answer'])
